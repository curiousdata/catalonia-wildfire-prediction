{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NetCDF datacube with chunking\n",
    "ds = xr.open_dataset(\"/Users/vladimir/catalonia-wildfire-prediction/data/IberFire.nc\", chunks=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 731GB\n",
      "Dimensions:                                        (y: 920, x: 1188, time: 6241)\n",
      "Coordinates:\n",
      "  * x                                              (x) float64 10kB 2.675e+06...\n",
      "  * y                                              (y) float64 7kB 2.492e+06 ...\n",
      "  * time                                           (time) datetime64[ns] 50kB ...\n",
      "Data variables: (12/261)\n",
      "    x_index                                        (y, x) uint16 2MB dask.array<chunksize=(920, 1188), meta=np.ndarray>\n",
      "    y_index                                        (y, x) uint16 2MB dask.array<chunksize=(920, 1188), meta=np.ndarray>\n",
      "    is_spain                                       (y, x) uint16 2MB dask.array<chunksize=(920, 1188), meta=np.ndarray>\n",
      "    is_fire                                        (time, y, x) uint8 7GB dask.array<chunksize=(1562, 230, 298), meta=np.ndarray>\n",
      "    is_near_fire                                   (time, y, x) uint8 7GB dask.array<chunksize=(1562, 230, 298), meta=np.ndarray>\n",
      "    x_coordinate                                   (y, x) float32 4MB dask.array<chunksize=(920, 1188), meta=np.ndarray>\n",
      "    ...                                             ...\n",
      "    LST                                            (time, y, x) float32 27GB dask.array<chunksize=(1042, 154, 198), meta=np.ndarray>\n",
      "    SWI_001                                        (time, y, x) float32 27GB dask.array<chunksize=(1042, 154, 198), meta=np.ndarray>\n",
      "    SWI_005                                        (time, y, x) float32 27GB dask.array<chunksize=(1042, 154, 198), meta=np.ndarray>\n",
      "    SWI_010                                        (time, y, x) float32 27GB dask.array<chunksize=(1042, 154, 198), meta=np.ndarray>\n",
      "    SWI_020                                        (time, y, x) float32 27GB dask.array<chunksize=(1042, 154, 198), meta=np.ndarray>\n",
      "    FWI                                            (time, y, x) float32 27GB dask.array<chunksize=(1042, 154, 198), meta=np.ndarray>\n",
      "Attributes: (12/17)\n",
      "    title:                IberFire\n",
      "    description:          Datacube centered in Spain with 1km x 1km spatial r...\n",
      "    dimensions:           (y: 920, x: 1188, time: 6241)\n",
      "    spatial_resolution:   1km x 1km\n",
      "    temporal_resolution:  Daily\n",
      "    start_date:           2007-12-01\n",
      "    ...                   ...\n",
      "    geospatial_x_max:     3861734.3466\n",
      "    geospatial_y_min:     1573195.9911000002\n",
      "    geospatial_y_max:     2492195.9911\n",
      "    author:               Julen Ercibengoa Calvo\n",
      "    author_contact:       julen.ercibengoa@gmail.com, julen.ercibengoa@teknik...\n",
      "    creation_date:        2025-04-04\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocess IberFire.nc into sharded ConvLSTM-ready samples without loading the cube in memory.\n",
    "\n",
    "Outputs\n",
    "- data/convlstm/{train,val,test}/shard_{k}.npz with:\n",
    "  - X: [N, T, C, H, W] float32\n",
    "  - y: [N, H, W] uint8 (next-day fire mask)\n",
    "- data/convlstm/{split}/manifest.parquet\n",
    "- data/convlstm/stats.json (per-variable mean/std computed on train split)\n",
    "- data/convlstm/config.json (pipeline config for training notebook)\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ----------------------\n",
    "# Config\n",
    "# ----------------------\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    # Input\n",
    "    \"input_path\": \"data/IberFire.nc\",  # or data/iberfire_catalonia.nc\n",
    "    # Features (must exist in dataset; add/remove as needed)\n",
    "    \"feature_vars\": [\n",
    "        \"LST\",       # Land Surface Temp\n",
    "        \"SWI_001\",   # Soil Water Index (example layers)\n",
    "        \"SWI_010\",\n",
    "        \"SWI_020\",\n",
    "        \"FWI\",       # Fire Weather Index\n",
    "    ],\n",
    "    \"label_var\": \"is_fire\",\n",
    "    # Time windowing\n",
    "    \"seq_len\": 7,          # T (days)\n",
    "    \"horizon\": 1,          # predict next day t+1\n",
    "    \"stride\": 1,           # slide by 1 day\n",
    "    # Spatial tiling\n",
    "    \"tile_h\": 128,\n",
    "    \"tile_w\": 128,\n",
    "    # Chunking for xarray/dask\n",
    "    \"chunks\": {\"time\": 256, \"y\": 256, \"x\": 256},\n",
    "    # Splits (inclusive ranges)\n",
    "    \"splits\": {\n",
    "        \"train\": [\"2008-01-01\", \"2018-12-31\"],\n",
    "        \"val\":   [\"2019-01-01\", \"2019-12-31\"],\n",
    "        \"test\":  [\"2020-01-01\", \"2021-12-31\"],\n",
    "    },\n",
    "    # Sharding\n",
    "    \"samples_per_shard\": 32,\n",
    "    \"out_dir\": \"data/convlstm\",\n",
    "    # NaN handling\n",
    "    \"nan_fill\": 0.0,\n",
    "}\n",
    "\n",
    "# ----------------------\n",
    "# Helpers\n",
    "# ----------------------\n",
    "\n",
    "def open_ds(path: str, chunks: Dict[str, int]) -> xr.Dataset:\n",
    "    ds = xr.open_dataset(path, chunks=chunks or \"auto\")\n",
    "    # Ensure needed dims present\n",
    "    for dim in (\"time\", \"y\", \"x\"):\n",
    "        if dim not in ds.dims:\n",
    "            raise ValueError(f\"Dataset missing required dim: {dim}\")\n",
    "    return ds\n",
    "\n",
    "def validate_vars(ds: xr.Dataset, feature_vars: List[str], label_var: str) -> List[str]:\n",
    "    present = []\n",
    "    missing = []\n",
    "    for v in feature_vars:\n",
    "        if v in ds.data_vars and set(ds[v].dims) >= {\"time\", \"y\", \"x\"}:\n",
    "            present.append(v)\n",
    "        else:\n",
    "            missing.append(v)\n",
    "    if missing:\n",
    "        print(f\"[warn] Missing feature vars (skipped): {missing}\")\n",
    "    if label_var not in ds.data_vars:\n",
    "        raise ValueError(f\"Label var '{label_var}' not found in dataset.\")\n",
    "    return present\n",
    "\n",
    "def time_index_for_range(time_index: xr.DataArray, start: str, end: str) -> np.ndarray:\n",
    "    t = pd.to_datetime(time_index.values)\n",
    "    mask = (t >= np.datetime64(start)) & (t <= np.datetime64(end))\n",
    "    return np.nonzero(mask)[0]\n",
    "\n",
    "def compute_norm_stats(ds: xr.Dataset, vars_: List[str], time_idx: np.ndarray) -> Dict[str, Dict[str, float]]:\n",
    "    stats = {}\n",
    "    # Restrict dataset to train time indices lazily\n",
    "    dstrain = ds.isel(time=time_idx)\n",
    "    for v in vars_:\n",
    "        arr = dstrain[v]\n",
    "        # Compute global mean/std across (time, y, x); skip NaNs\n",
    "        mean = arr.mean(dim=(\"time\", \"y\", \"x\"), skipna=True).compute().item()\n",
    "        std = arr.std(dim=(\"time\", \"y\", \"x\"), skipna=True).compute().item()\n",
    "        # Avoid div by zero\n",
    "        std = float(std) if std and std > 1e-6 else 1.0\n",
    "        stats[v] = {\"mean\": float(mean), \"std\": float(std)}\n",
    "    return stats\n",
    "\n",
    "def iter_time_windows(all_t_idx: np.ndarray, seq_len: int, horizon: int, stride: int) -> Iterable[Tuple[int, int, int]]:\n",
    "    \"\"\"\n",
    "    Yields (t_start, t_end_exclusive, t_label) indices.\n",
    "    \"\"\"\n",
    "    last_label = all_t_idx[-1]\n",
    "    for t0 in all_t_idx[::stride]:\n",
    "        t_end = t0 + seq_len  # exclusive\n",
    "        t_label = t0 + seq_len - 1 + horizon\n",
    "        if t_label <= last_label:\n",
    "            yield (t0, t_end, t_label)\n",
    "\n",
    "def iter_tiles(y_size: int, x_size: int, tile_h: int, tile_w: int) -> Iterable[Tuple[int, int]]:\n",
    "    for y0 in range(0, y_size, tile_h):\n",
    "        for x0 in range(0, x_size, tile_w):\n",
    "            yield y0, x0\n",
    "\n",
    "def extract_window(\n",
    "    ds: xr.Dataset,\n",
    "    feature_vars: List[str],\n",
    "    label_var: str,\n",
    "    t0: int,\n",
    "    t_end: int,\n",
    "    t_label: int,\n",
    "    y0: int,\n",
    "    x0: int,\n",
    "    tile_h: int,\n",
    "    tile_w: int,\n",
    "    stats: Dict[str, Dict[str, float]],\n",
    "    nan_fill: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ys = slice(y0, min(y0 + tile_h, ds.dims[\"y\"]))\n",
    "    xs = slice(x0, min(x0 + tile_w, ds.dims[\"x\"]))\n",
    "    ts = slice(t0, t_end)\n",
    "\n",
    "    # Features -> [T, C, H, W]\n",
    "    feat_arrays = []\n",
    "    for v in feature_vars:\n",
    "        v_arr = ds[v].isel(time=ts, y=ys, x=xs).astype(\"float32\")\n",
    "        mean, std = stats[v][\"mean\"], stats[v][\"std\"]\n",
    "        v_arr = (v_arr - mean) / std\n",
    "        v_arr = v_arr.fillna(nan_fill)\n",
    "        # to numpy (compute) lazily per window\n",
    "        v_np = np.asarray(v_arr)  # dask will compute here\n",
    "        # -> [T, H, W]\n",
    "        feat_arrays.append(v_np)\n",
    "    # Stack channels\n",
    "    X = np.stack(feat_arrays, axis=1)  # [T, C, H, W]\n",
    "\n",
    "    # Label (next-day fire mask) -> [H, W] uint8\n",
    "    y_arr = ds[label_var].isel(time=t_label, y=ys, x=xs).astype(\"uint8\").fillna(0)\n",
    "    y = np.asarray(y_arr)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def write_shard(out_dir: Path, split: str, shard_idx: int, Xs: List[np.ndarray], ys: List[np.ndarray]) -> Path:\n",
    "    out_dir_split = out_dir / split\n",
    "    out_dir_split.mkdir(parents=True, exist_ok=True)\n",
    "    shard_path = out_dir_split / f\"shard_{shard_idx:06d}.npz\"\n",
    "    X = np.stack(Xs, axis=0)  # [N, T, C, H, W]\n",
    "    y = np.stack(ys, axis=0)  # [N, H, W]\n",
    "    np.savez_compressed(shard_path, X=X, y=y)\n",
    "    return shard_path\n",
    "\n",
    "# ----------------------\n",
    "# Main\n",
    "# ----------------------\n",
    "\n",
    "def run(cfg: Dict):\n",
    "    out_dir = Path(cfg[\"out_dir\"])\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"[info] Opening dataset...\")\n",
    "    ds = open_ds(cfg[\"input_path\"], cfg[\"chunks\"])\n",
    "    feat_vars = validate_vars(ds, cfg[\"feature_vars\"], cfg[\"label_var\"])\n",
    "    print(f\"[info] Using features: {feat_vars}; label: {cfg['label_var']}\")\n",
    "    # Persist chunking for faster repeated reads\n",
    "    ds = ds[feat_vars + [cfg[\"label_var\"]]].chunk(cfg[\"chunks\"])\n",
    "\n",
    "    # Split indices\n",
    "    t_values = ds[\"time\"].values\n",
    "    split_idx = {\n",
    "        k: time_index_for_range(ds[\"time\"], v[0], v[1]) for k, v in cfg[\"splits\"].items()\n",
    "    }\n",
    "\n",
    "    # Compute normalization stats on train split\n",
    "    print(\"[info] Computing normalization stats (train split)...\")\n",
    "    stats = compute_norm_stats(ds, feat_vars, split_idx[\"train\"])\n",
    "    with open(out_dir / \"stats.json\", \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    with open(out_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "    for split, idx in split_idx.items():\n",
    "        if len(idx) == 0:\n",
    "            print(f\"[warn] No time indices for split {split}, skipping.\")\n",
    "            continue\n",
    "        print(f\"[info] Generating {split} shards...\")\n",
    "        shard_idx = 0\n",
    "        X_bucket, y_bucket = [], []\n",
    "        manifest_rows = []\n",
    "\n",
    "        y_size, x_size = ds.dims[\"y\"], ds.dims[\"x\"]\n",
    "        windows = list(iter_time_windows(idx, cfg[\"seq_len\"], cfg[\"horizon\"], cfg[\"stride\"]))\n",
    "        total_windows = len(windows) * ((y_size + cfg[\"tile_h\"] - 1) // cfg[\"tile_h\"]) * ((x_size + cfg[\"tile_w\"] - 1) // cfg[\"tile_w\"])\n",
    "        done = 0\n",
    "\n",
    "        for (t0, t_end, t_label) in windows:\n",
    "            for (y0, x0) in iter_tiles(y_size, x_size, cfg[\"tile_h\"], cfg[\"tile_w\"]):\n",
    "                try:\n",
    "                    X, y = extract_window(\n",
    "                        ds, feat_vars, cfg[\"label_var\"],\n",
    "                        t0, t_end, t_label,\n",
    "                        y0, x0, cfg[\"tile_h\"], cfg[\"tile_w\"],\n",
    "                        stats, cfg[\"nan_fill\"]\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"[warn] failed window t0={t0}, y0={y0}, x0={x0}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                X_bucket.append(X)\n",
    "                y_bucket.append(y)\n",
    "                if len(X_bucket) >= cfg[\"samples_per_shard\"]:\n",
    "                    shard_path = write_shard(out_dir, split, shard_idx, X_bucket, y_bucket)\n",
    "                    manifest_rows.append({\n",
    "                        \"shard\": shard_path.name,\n",
    "                        \"num_samples\": len(X_bucket),\n",
    "                        \"t_start\": str(pd.to_datetime(t_values[t0])),\n",
    "                        \"t_label\": str(pd.to_datetime(t_values[t_label])),\n",
    "                    })\n",
    "                    shard_idx += 1\n",
    "                    X_bucket, y_bucket = [], []\n",
    "\n",
    "                done += 1\n",
    "                if done % 50 == 0:\n",
    "                    print(f\"[info] {split}: {done}/{total_windows} samples queued...\")\n",
    "\n",
    "        # Flush remainder\n",
    "        if X_bucket:\n",
    "            shard_path = write_shard(out_dir, split, shard_idx, X_bucket, y_bucket)\n",
    "            manifest_rows.append({\n",
    "                \"shard\": shard_path.name,\n",
    "                \"num_samples\": len(X_bucket),\n",
    "                \"t_start\": str(pd.to_datetime(t_values[windows[-1][0]])),\n",
    "                \"t_label\": str(pd.to_datetime(t_values[windows[-1][2]])),\n",
    "            })\n",
    "\n",
    "        # Save manifest\n",
    "        if manifest_rows:\n",
    "            man_df = pd.DataFrame(manifest_rows)\n",
    "            man_df.to_parquet(out_dir / split / \"manifest.parquet\", index=False)\n",
    "            print(f\"[info] Wrote {len(manifest_rows)} shards to {out_dir / split}\")\n",
    "\n",
    "    print(\"[done] Preprocessing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input\", type=str, default=DEFAULT_CONFIG[\"input_path\"])\n",
    "    parser.add_argument(\"--out\", type=str, default=DEFAULT_CONFIG[\"out_dir\"])\n",
    "    parser.add_argument(\"--seq-len\", type=int, default=DEFAULT_CONFIG[\"seq_len\"])\n",
    "    parser.add_argument(\"--horizon\", type=int, default=DEFAULT_CONFIG[\"horizon\"])\n",
    "    parser.add_argument(\"--stride\", type=int, default=DEFAULT_CONFIG[\"stride\"])\n",
    "    parser.add_argument(\"--tile-h\", type=int, default=DEFAULT_CONFIG[\"tile_h\"])\n",
    "    parser.add_argument(\"--tile-w\", type=int, default=DEFAULT_CONFIG[\"tile_w\"])\n",
    "    parser.add_argument(\"--samples-per-shard\", type=int, default=DEFAULT_CONFIG[\"samples_per_shard\"])\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cfg = DEFAULT_CONFIG.copy()\n",
    "    cfg[\"input_path\"] = args.input\n",
    "    cfg[\"out_dir\"] = args.out\n",
    "    cfg[\"seq_len\"] = args.seq_len\n",
    "    cfg[\"horizon\"] = args.horizon\n",
    "    cfg[\"stride\"] = args.stride\n",
    "    cfg[\"tile_h\"] = args.tile_h\n",
    "    cfg[\"tile_w\"] = args.tile_w\n",
    "    cfg[\"samples_per_shard\"] = args.samples_per_shard\n",
    "\n",
    "    run(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
