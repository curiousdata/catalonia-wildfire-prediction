{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports and config\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = \"data/iberfire.nc\"  # adjust if needed\n",
    "OUT_NPZ = Path(\"data/minimal_cnn_samples.npz\")\n",
    "\n",
    "# choose very few variables and short time range\n",
    "FEATURE_VARS = [\"CLC_2006_forest_proportion\", \"wind_speed_mean\", \"t2m_mean\", \"RH_mean\", \"total_precipitation_mean\", \"is_holiday\"]  # change to actual names in ds\n",
    "LABEL_VAR = \"is_near_fire\"\n",
    "\n",
    "TIME_START = \"2018-06-01\"\n",
    "TIME_END   = \"2020-08-31\" \n",
    "SPATIAL_DOWNSAMPLE = 4      # keep every 4th pixel in y and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 123GB\n",
      "Dimensions:                     (y: 920, x: 1188, time: 6241)\n",
      "Coordinates:\n",
      "  * x                           (x) float64 10kB 2.675e+06 ... 3.862e+06\n",
      "  * y                           (y) float64 7kB 2.492e+06 ... 1.573e+06\n",
      "  * time                        (time) datetime64[ns] 50kB 2007-12-01 ... 202...\n",
      "Data variables:\n",
      "    CLC_2006_forest_proportion  (y, x) float32 4MB ...\n",
      "    wind_speed_mean             (time, y, x) float32 27GB ...\n",
      "    t2m_mean                    (time, y, x) float32 27GB ...\n",
      "    RH_mean                     (time, y, x) float32 27GB ...\n",
      "    total_precipitation_mean    (time, y, x) float32 27GB ...\n",
      "    is_holiday                  (time, y, x) uint8 7GB ...\n",
      "    is_near_fire                (time, y, x) uint8 7GB ...\n",
      "Attributes: (12/17)\n",
      "    title:                IberFire\n",
      "    description:          Datacube centered in Spain with 1km x 1km spatial r...\n",
      "    dimensions:           (y: 920, x: 1188, time: 6241)\n",
      "    spatial_resolution:   1km x 1km\n",
      "    temporal_resolution:  Daily\n",
      "    start_date:           2007-12-01\n",
      "    ...                   ...\n",
      "    geospatial_x_max:     3861734.3466\n",
      "    geospatial_y_min:     1573195.9911000002\n",
      "    geospatial_y_max:     2492195.9911\n",
      "    author:               Julen Ercibengoa Calvo\n",
      "    author_contact:       julen.ercibengoa@gmail.com, julen.ercibengoa@teknik...\n",
      "    creation_date:        2025-04-04\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: open dataset & inspect\n",
    "ds = xr.open_dataset(DATA_PATH)  # no chunks for now; we keep this small\n",
    "\n",
    "print(ds[FEATURE_VARS + [LABEL_VAR]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_41219/2112922127.py:3: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(\"Selected time steps:\", time_sel.dims[\"time\"])\n",
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_41219/2112922127.py:8: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for t_idx in range(time_sel.dims[\"time\"]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected time steps: 823\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m frame:\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in dataset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     arr = \u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m  \u001b[38;5;66;03m# [H,W]\u001b[39;00m\n\u001b[32m     18\u001b[39m     feat_arrays.append(arr)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# stack channels -> [C,H,W]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/dataarray.py:797\u001b[39m, in \u001b[36mDataArray.values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m    786\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    787\u001b[39m \u001b[33;03m    The array's data converted to numpy.ndarray.\u001b[39;00m\n\u001b[32m    788\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    795\u001b[39m \u001b[33;03m    to this array may be reflected in the DataArray as well.\u001b[39;00m\n\u001b[32m    796\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/variable.py:536\u001b[39m, in \u001b[36mVariable.values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m    535\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/variable.py:316\u001b[39m, in \u001b[36m_as_array_or_item\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_as_array_or_item\u001b[39m(data):\n\u001b[32m    303\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[32m    305\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m \u001b[33;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m    318\u001b[39m         kind = data.dtype.kind\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/indexing.py:509\u001b[39m, in \u001b[36mExplicitlyIndexed.__array__\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__array__\u001b[39m(\n\u001b[32m    505\u001b[39m     \u001b[38;5;28mself\u001b[39m, dtype: np.typing.DTypeLike = \u001b[38;5;28;01mNone\u001b[39;00m, /, *, copy: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    506\u001b[39m ) -> np.ndarray:\n\u001b[32m    507\u001b[39m     \u001b[38;5;66;03m# Leave casting to an array up to the underlying array type.\u001b[39;00m\n\u001b[32m    508\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m Version(np.__version__) >= Version(\u001b[33m\"\u001b[39m\u001b[33m2.0.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype=dtype, copy=copy)\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(\u001b[38;5;28mself\u001b[39m.get_duck_array(), dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/indexing.py:843\u001b[39m, in \u001b[36mMemoryCachedArray.get_duck_array\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.array.get_duck_array()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/indexing.py:840\u001b[39m, in \u001b[36mMemoryCachedArray._ensure_cached\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m     \u001b[38;5;28mself\u001b[39m.array = as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/indexing.py:797\u001b[39m, in \u001b[36mCopyOnWriteArray.get_duck_array\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/indexing.py:652\u001b[39m, in \u001b[36mLazilyIndexedArray.get_duck_array\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    648\u001b[39m     array = apply_indexer(\u001b[38;5;28mself\u001b[39m.array, \u001b[38;5;28mself\u001b[39m.key)\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    650\u001b[39m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[32m    651\u001b[39m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m     array = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[32m    655\u001b[39m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/backends/netCDF4_.py:108\u001b[39m, in \u001b[36mNetCDF4ArrayWrapper.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexingSupport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/indexing.py:1021\u001b[39m, in \u001b[36mexplicit_indexing_adapter\u001b[39m\u001b[34m(key, shape, indexing_support, raw_indexing_method)\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[32m   1000\u001b[39m \n\u001b[32m   1001\u001b[39m \u001b[33;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1018\u001b[39m \u001b[33;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[32m   1019\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1020\u001b[39m raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m result = \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices.tuple:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;66;03m# index the loaded duck array\u001b[39;00m\n\u001b[32m   1024\u001b[39m     indexable = as_indexable(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/backends/netCDF4_.py:121\u001b[39m, in \u001b[36mNetCDF4ArrayWrapper._getitem\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.datastore.lock:\n\u001b[32m    120\u001b[39m         original_array = \u001b[38;5;28mself\u001b[39m.get_array(needs_lock=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         array = \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# indexer is used before the data is loaded from disk.\u001b[39;00m\n\u001b[32m    126\u001b[39m     msg = (\n\u001b[32m    127\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe indexing operation you are attempting to perform \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mis not valid on netCDF4.Variable object. Try loading \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myour data into memory first by calling .load().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 3: extract small tensor dataset\n",
    "time_sel = ds.sel(time=slice(TIME_START, TIME_END))\n",
    "print(\"Selected time steps:\", time_sel.dims[\"time\"])\n",
    "\n",
    "Xs = []\n",
    "ys = []\n",
    "\n",
    "for t_idx in range(time_sel.dims[\"time\"]):\n",
    "    # select at time index\n",
    "    frame = time_sel.isel(time=t_idx)\n",
    "\n",
    "    # features: list of [H,W]\n",
    "    feat_arrays = []\n",
    "    for v in FEATURE_VARS:\n",
    "        if v not in frame:\n",
    "            raise ValueError(f\"Variable {v} not found in dataset\")\n",
    "        arr = frame[v].values  # [H,W]\n",
    "        feat_arrays.append(arr)\n",
    "\n",
    "    # stack channels -> [C,H,W]\n",
    "    X = np.stack(feat_arrays, axis=0)\n",
    "\n",
    "    # label\n",
    "    if LABEL_VAR not in frame:\n",
    "        raise ValueError(f\"Label variable {LABEL_VAR} not found\")\n",
    "    y = frame[LABEL_VAR].values.astype(\"float32\")  # [H,W]\n",
    "\n",
    "    # optional: spatial downsample\n",
    "    X = X[:, ::SPATIAL_DOWNSAMPLE, ::SPATIAL_DOWNSAMPLE]\n",
    "    y = y[::SPATIAL_DOWNSAMPLE, ::SPATIAL_DOWNSAMPLE]\n",
    "\n",
    "    Xs.append(X)\n",
    "    ys.append(y)\n",
    "\n",
    "Xs = np.stack(Xs, axis=0)  # [N,C,H,W]\n",
    "ys = np.stack(ys, axis=0)  # [N,H,W]\n",
    "\n",
    "print(\"Xs shape:\", Xs.shape, \"ys shape:\", ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 4: simple normalization + save to disk\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# normalize each channel by global mean/std over this mini-dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m C = \u001b[43mXs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(C):\n\u001b[32m      5\u001b[39m     mean = Xs[:, c].mean()\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Cell 4: simple normalization + save to disk\n",
    "# normalize each channel by global mean/std over this mini-dataset\n",
    "C = Xs.shape[1]\n",
    "for c in range(C):\n",
    "    mean = Xs[:, c].mean()\n",
    "    std = Xs[:, c].std()\n",
    "    if std < 1e-6:\n",
    "        std = 1.0\n",
    "    Xs[:, c] = (Xs[:, c] - mean) / std\n",
    "\n",
    "# binarize label if needed (0/1)\n",
    "ys_bin = (ys > 0.5).astype(\"float32\")\n",
    "\n",
    "OUT_NPZ.parent.mkdir(parents=True, exist_ok=True)\n",
    "np.savez_compressed(OUT_NPZ, X=Xs, y=ys_bin)\n",
    "print(\"Saved to\", OUT_NPZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 457\n",
      "Sample X shape: torch.Size([6, 64, 71]) y shape: torch.Size([1, 64, 71])\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: PyTorch dataset for the npz file\n",
    "class MinimalIberFireDataset(Dataset):\n",
    "    def __init__(self, npz_path):\n",
    "        data = np.load(npz_path)\n",
    "        self.X = data[\"X\"]          # [N,C,H,W]\n",
    "        self.y = data[\"y\"]          # [N,H,W]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.from_numpy(self.X[idx]).float()        # [C,H,W]\n",
    "        y = torch.from_numpy(self.y[idx]).float()        # [H,W]\n",
    "        return X, y.unsqueeze(0)                         # [1,H,W]\n",
    "\n",
    "dataset = MinimalIberFireDataset(OUT_NPZ)\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "X0, y0 = dataset[0]\n",
    "print(\"Sample X shape:\", X0.shape, \"y shape:\", y0.shape)\n",
    "\n",
    "# Cell 6: create train/val splits and DataLoaders\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "N = len(dataset)\n",
    "n_train = int(0.8 * N)\n",
    "n_val = N - n_train\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyFireCNN(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: tiny CNN model\n",
    "class TinyFireCNN(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, 1),  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # [B,1,H,W]\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "in_channels = X0.shape[0]\n",
    "model = TinyFireCNN(in_channels).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss=0.6593 val_loss=0.5962 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n",
      "Epoch 2 | train_loss=0.5094 val_loss=0.4145 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n",
      "Epoch 3 | train_loss=0.3222 val_loss=0.2393 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n",
      "Epoch 4 | train_loss=0.1803 val_loss=0.1345 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n",
      "Epoch 5 | train_loss=0.1037 val_loss=0.0835 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: training loop\n",
    "def pixel_metrics(logits, y, thr=0.5):\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= thr).float()\n",
    "        y = y.float()\n",
    "        tp = (preds * y).sum().item()\n",
    "        fp = (preds * (1 - y)).sum().item()\n",
    "        fn = ((1 - preds) * y).sum().item()\n",
    "        tn = (((1 - preds) * (1 - y))).sum().item()\n",
    "        eps = 1e-8\n",
    "        prec = tp / (tp + fp + eps)\n",
    "        rec = tp / (tp + fn + eps)\n",
    "        f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= max(1, len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    acc_sum = prec_sum = rec_sum = f1_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item()\n",
    "            a, p, r, f1 = pixel_metrics(logits, y)\n",
    "            acc_sum += a; prec_sum += p; rec_sum += r; f1_sum += f1\n",
    "    val_loss /= max(1, len(val_loader))\n",
    "    acc = acc_sum / max(1, len(val_loader))\n",
    "    prec = prec_sum / max(1, len(val_loader))\n",
    "    rec = rec_sum / max(1, len(val_loader))\n",
    "    f1 = f1_sum / max(1, len(val_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch} | train_loss={train_loss:.4f} \"\n",
    "          f\"val_loss={val_loss:.4f} acc={acc:.3f} prec={prec:.3f} rec={rec:.3f} f1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pixels: 2076608 positives: 8787 negatives: 2067821 pos_ratio: 0.004231419699818165\n"
     ]
    }
   ],
   "source": [
    "# Check fire prevalence in the mini dataset\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(\"data/minimal_cnn_samples.npz\")\n",
    "y = data[\"y\"]  # [N,H,W], already binarized\n",
    "\n",
    "pos = (y == 1).sum()\n",
    "neg = (y == 0).sum()\n",
    "print(\"Total pixels:\", y.size, \"positives:\", pos, \"negatives:\", neg, \"pos_ratio:\", pos / y.size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
