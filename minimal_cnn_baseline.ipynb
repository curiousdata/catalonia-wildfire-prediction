{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports and config\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"data/iberfire.nc\"  # adjust if needed\n",
    "# NEW: output directory structure instead of single file\n",
    "OUT_DIR = Path(\"data/minimal_cnn_sharded\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# choose very few variables and short time range\n",
    "FEATURE_VARS = [\"CLC_2006_forest_proportion\", \"wind_speed_mean\", \"t2m_mean\", \"RH_mean\", \"total_precipitation_mean\", \"is_holiday\"]  # change to actual names in ds\n",
    "LABEL_VAR = \"is_near_fire\"\n",
    "SAMPLES_PER_SHARD = 64  # write every 64 time steps\n",
    "TIME_START = \"2018-06-01\"\n",
    "TIME_END   = \"2020-08-31\" \n",
    "SPATIAL_DOWNSAMPLE = 4      # keep every 4th pixel in y and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 123GB\n",
      "Dimensions:                     (y: 920, x: 1188, time: 6241)\n",
      "Coordinates:\n",
      "  * x                           (x) float64 10kB 2.675e+06 ... 3.862e+06\n",
      "  * y                           (y) float64 7kB 2.492e+06 ... 1.573e+06\n",
      "  * time                        (time) datetime64[ns] 50kB 2007-12-01 ... 202...\n",
      "Data variables:\n",
      "    CLC_2006_forest_proportion  (y, x) float32 4MB dask.array<chunksize=(256, 256), meta=np.ndarray>\n",
      "    wind_speed_mean             (time, y, x) float32 27GB dask.array<chunksize=(64, 256, 256), meta=np.ndarray>\n",
      "    t2m_mean                    (time, y, x) float32 27GB dask.array<chunksize=(64, 256, 256), meta=np.ndarray>\n",
      "    RH_mean                     (time, y, x) float32 27GB dask.array<chunksize=(64, 256, 256), meta=np.ndarray>\n",
      "    total_precipitation_mean    (time, y, x) float32 27GB dask.array<chunksize=(64, 256, 256), meta=np.ndarray>\n",
      "    is_holiday                  (time, y, x) uint8 7GB dask.array<chunksize=(64, 256, 256), meta=np.ndarray>\n",
      "    is_near_fire                (time, y, x) uint8 7GB dask.array<chunksize=(64, 256, 256), meta=np.ndarray>\n",
      "Attributes: (12/17)\n",
      "    title:                IberFire\n",
      "    description:          Datacube centered in Spain with 1km x 1km spatial r...\n",
      "    dimensions:           (y: 920, x: 1188, time: 6241)\n",
      "    spatial_resolution:   1km x 1km\n",
      "    temporal_resolution:  Daily\n",
      "    start_date:           2007-12-01\n",
      "    ...                   ...\n",
      "    geospatial_x_max:     3861734.3466\n",
      "    geospatial_y_min:     1573195.9911000002\n",
      "    geospatial_y_max:     2492195.9911\n",
      "    author:               Julen Ercibengoa Calvo\n",
      "    author_contact:       julen.ercibengoa@gmail.com, julen.ercibengoa@teknik...\n",
      "    creation_date:        2025-04-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_76922/1061696706.py:2: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(DATA_PATH, chunks={'time': 64, 'y': 256, 'x': 256})  # use dask for lazy loading\n",
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_76922/1061696706.py:2: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(DATA_PATH, chunks={'time': 64, 'y': 256, 'x': 256})  # use dask for lazy loading\n",
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_76922/1061696706.py:2: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 64. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(DATA_PATH, chunks={'time': 64, 'y': 256, 'x': 256})  # use dask for lazy loading\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: open dataset & inspect\n",
    "ds = xr.open_dataset(DATA_PATH, chunks={'time': 64, 'y': 256, 'x': 256})  # use dask for lazy loading\n",
    "\n",
    "print(ds[FEATURE_VARS + [LABEL_VAR]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_76922/705199705.py:4: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Selected time steps: {time_sel.dims['time']}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected time steps: 823\n",
      "Computing normalization stats...\n",
      "  CLC_2006_forest_proportion: mean=0.1537, std=0.2813\n",
      "  wind_speed_mean: mean=2.3934, std=1.1843\n",
      "  t2m_mean: mean=15.2473, std=7.3298\n",
      "  RH_mean: mean=67.4048, std=17.0030\n",
      "  total_precipitation_mean: mean=0.9370, std=2.5542\n",
      "  is_holiday: mean=0.2993, std=0.4579\n",
      "Stats saved to data/minimal_cnn_sharded/stats.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: extract small tensor dataset\n",
    "\n",
    "time_sel = ds.sel(time=slice(TIME_START, TIME_END))\n",
    "print(f\"Selected time steps: {time_sel.dims['time']}\")\n",
    "\n",
    "# NEW: compute stats efficiently using xarray reductions\n",
    "print(\"Computing normalization stats...\")\n",
    "stats = {}\n",
    "for v in FEATURE_VARS:\n",
    "    var = time_sel[v]\n",
    "    \n",
    "    # handle spatial-only variables (no time dimension)\n",
    "    if \"time\" not in var.dims:\n",
    "        # broadcast to time dimension for consistent stats computation\n",
    "        var = var.expand_dims(time=time_sel[\"time\"])\n",
    "    \n",
    "    # forward/backward fill to handle NaNs, then compute stats\n",
    "    var_filled = var.ffill(\"time\").bfill(\"time\")\n",
    "    \n",
    "    # compute mean and std (xarray will handle chunked computation)\n",
    "    mean = float(var_filled.mean(skipna=True).compute())\n",
    "    std = float(var_filled.std(skipna=True).compute())\n",
    "    \n",
    "    stats[v] = {\n",
    "        \"mean\": mean,\n",
    "        \"std\": std if std > 1e-6 else 1.0\n",
    "    }\n",
    "    print(f\"  {v}: mean={mean:.4f}, std={std:.4f}\")\n",
    "\n",
    "# NEW: save stats for later use\n",
    "import json\n",
    "with open(OUT_DIR / \"stats.json\", \"w\") as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"Stats saved to {OUT_DIR / 'stats.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 823 time steps...\n",
      "  Processing time step 0/823...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_76922/2922153908.py:11: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Processing {time_sel.dims['time']} time steps...\")\n",
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_76922/2922153908.py:13: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for t_idx in range(time_sel.dims['time']):\n",
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_76922/2922153908.py:16: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  Processing time step {t_idx}/{time_sel.dims['time']}...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing time step 50/823...\n",
      "    Wrote shard_000000.npz\n",
      "  Processing time step 100/823...\n",
      "    Wrote shard_000001.npz\n",
      "  Processing time step 150/823...\n",
      "    Wrote shard_000002.npz\n",
      "  Processing time step 200/823...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: extract and write shards incrementally\n",
    "\n",
    "train_dir = OUT_DIR / \"train\"\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shard_idx = 0\n",
    "X_bucket = []\n",
    "y_bucket = []\n",
    "manifest = []\n",
    "\n",
    "print(f\"Processing {time_sel.dims['time']} time steps...\")\n",
    "\n",
    "for t_idx in range(time_sel.dims['time']):\n",
    "    # Progress indicator every 50 steps\n",
    "    if t_idx % 50 == 0:\n",
    "        print(f\"  Processing time step {t_idx}/{time_sel.dims['time']}...\")\n",
    "    \n",
    "    # Extract single time slice (small, fits in RAM)\n",
    "    frame = time_sel.isel(time=t_idx)\n",
    "    \n",
    "    # Extract features -> [C,H,W]\n",
    "    feat_arrays = []\n",
    "    for v in FEATURE_VARS:\n",
    "        if v not in frame:\n",
    "            raise ValueError(f\"Variable {v} not found in dataset\")\n",
    "        arr = frame[v].values  # load small slice into RAM\n",
    "        feat_arrays.append(arr)\n",
    "    \n",
    "    X = np.stack(feat_arrays, axis=0)  # [C,H,W]\n",
    "    \n",
    "    # Normalize using pre-computed stats\n",
    "    for c, v in enumerate(FEATURE_VARS):\n",
    "        X[c] = (X[c] - stats[v][\"mean\"]) / stats[v][\"std\"]\n",
    "    \n",
    "    # Spatial downsample\n",
    "    X = X[:, ::SPATIAL_DOWNSAMPLE, ::SPATIAL_DOWNSAMPLE]\n",
    "    \n",
    "    # Extract label\n",
    "    if LABEL_VAR not in frame:\n",
    "        raise ValueError(f\"Label variable {LABEL_VAR} not found\")\n",
    "    y = frame[LABEL_VAR].values.astype(\"float32\")  # [H,W]\n",
    "    y = y[::SPATIAL_DOWNSAMPLE, ::SPATIAL_DOWNSAMPLE]\n",
    "    \n",
    "    # Binarize\n",
    "    y_bin = (y > 0.5).astype(\"float32\")\n",
    "    \n",
    "    # Add to bucket\n",
    "    X_bucket.append(X)\n",
    "    y_bucket.append(y_bin)\n",
    "    \n",
    "    # Write shard when bucket is full\n",
    "    if len(X_bucket) == SAMPLES_PER_SHARD:\n",
    "        shard_path = train_dir / f\"shard_{shard_idx:06d}.npz\"\n",
    "        np.savez_compressed(\n",
    "            shard_path,\n",
    "            X=np.stack(X_bucket, axis=0),\n",
    "            y=np.stack(y_bucket, axis=0)\n",
    "        )\n",
    "        manifest.append({\n",
    "            \"shard\": shard_path.name,\n",
    "            \"num_samples\": len(X_bucket)\n",
    "        })\n",
    "        print(f\"    Wrote {shard_path.name}\")\n",
    "        \n",
    "        # Clear bucket\n",
    "        X_bucket = []\n",
    "        y_bucket = []\n",
    "        shard_idx += 1\n",
    "\n",
    "# Write leftover samples\n",
    "if X_bucket:\n",
    "    shard_path = train_dir / f\"shard_{shard_idx:06d}.npz\"\n",
    "    np.savez_compressed(\n",
    "        shard_path,\n",
    "        X=np.stack(X_bucket, axis=0),\n",
    "        y=np.stack(y_bucket, axis=0)\n",
    "    )\n",
    "    manifest.append({\n",
    "        \"shard\": shard_path.name,\n",
    "        \"num_samples\": len(X_bucket)\n",
    "    })\n",
    "    print(f\"    Wrote {shard_path.name} (final)\")\n",
    "\n",
    "# Write manifest\n",
    "pd.DataFrame(manifest).to_parquet(train_dir / \"manifest.parquet\", index=False)\n",
    "print(f\"\\nDone! Wrote {shard_idx + 1} shards to {train_dir}\")\n",
    "print(f\"Total samples: {sum(m['num_samples'] for m in manifest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 457\n",
      "Sample X shape: torch.Size([6, 64, 71]) y shape: torch.Size([1, 64, 71])\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: PyTorch Dataset for sharded data\n",
    "\n",
    "class ShardedIberFireDataset(Dataset):\n",
    "    \"\"\"Streams samples from multiple .npz shards without loading all into RAM.\"\"\"\n",
    "    \n",
    "    def __init__(self, split_dir):\n",
    "        self.split_dir = Path(split_dir)\n",
    "        \n",
    "        # Load manifest to know which shards exist\n",
    "        manifest_path = self.split_dir / \"manifest.parquet\"\n",
    "        if not manifest_path.exists():\n",
    "            raise FileNotFoundError(f\"Manifest not found: {manifest_path}\")\n",
    "        \n",
    "        df = pd.read_parquet(manifest_path)\n",
    "        self.shards = [\n",
    "            (self.split_dir / row.shard, int(row.num_samples))\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Compute cumulative sum for global indexing\n",
    "        self.cum = np.cumsum([n for _, n in self.shards])\n",
    "        \n",
    "        # Cache for memory-mapped files (per worker)\n",
    "        self._cache = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.cum[-1])\n",
    "    \n",
    "    def _open_npz(self, path):\n",
    "        \"\"\"Open npz with memory mapping, cached per worker.\"\"\"\n",
    "        wid = torch.utils.data.get_worker_info()\n",
    "        worker_id = wid.id if wid else -1\n",
    "        key = (worker_id, str(path))\n",
    "        \n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = np.load(path, mmap_mode=\"r\", allow_pickle=False)\n",
    "        \n",
    "        return self._cache[key]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Find which shard contains this index\n",
    "        shard_idx = int(np.searchsorted(self.cum, idx, side=\"right\"))\n",
    "        \n",
    "        # Compute local index within shard\n",
    "        base = 0 if shard_idx == 0 else int(self.cum[shard_idx - 1])\n",
    "        local_idx = int(idx - base)\n",
    "        \n",
    "        # Open shard and extract sample\n",
    "        path, _ = self.shards[shard_idx]\n",
    "        f = self._open_npz(path)\n",
    "        \n",
    "        X = torch.from_numpy(f[\"X\"][local_idx]).float()  # [C,H,W]\n",
    "        y = torch.from_numpy(f[\"y\"][local_idx]).float()  # [H,W]\n",
    "        \n",
    "        return X, y.unsqueeze(0)  # [1,H,W] for consistency\n",
    "\n",
    "# Create dataset\n",
    "dataset = ShardedIberFireDataset(OUT_DIR / \"train\")\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# Test one sample\n",
    "X0, y0 = dataset[0]\n",
    "print(\"Sample X shape:\", X0.shape, \"y shape:\", y0.shape)\n",
    "\n",
    "# Cell 6: create train/val splits and DataLoaders\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "N = len(dataset)\n",
    "n_train = int(0.8 * N)\n",
    "n_val = N - n_train\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyFireCNN(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: tiny CNN model\n",
    "class TinyFireCNN(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, 1),  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # [B,1,H,W]\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "in_channels = X0.shape[0]\n",
    "model = TinyFireCNN(in_channels).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss=0.6593 val_loss=0.5962 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n",
      "Epoch 2 | train_loss=0.5094 val_loss=0.4145 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n",
      "Epoch 3 | train_loss=0.3222 val_loss=0.2393 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n",
      "Epoch 4 | train_loss=0.1803 val_loss=0.1345 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n",
      "Epoch 5 | train_loss=0.1037 val_loss=0.0835 acc=0.995 prec=0.000 rec=0.000 f1=0.000\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: training loop\n",
    "def pixel_metrics(logits, y, thr=0.5):\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= thr).float()\n",
    "        y = y.float()\n",
    "        tp = (preds * y).sum().item()\n",
    "        fp = (preds * (1 - y)).sum().item()\n",
    "        fn = ((1 - preds) * y).sum().item()\n",
    "        tn = (((1 - preds) * (1 - y))).sum().item()\n",
    "        eps = 1e-8\n",
    "        prec = tp / (tp + fp + eps)\n",
    "        rec = tp / (tp + fn + eps)\n",
    "        f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "EPOCHS = 25\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    print('training on device:', device)\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= max(1, len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    acc_sum = prec_sum = rec_sum = f1_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item()\n",
    "            a, p, r, f1 = pixel_metrics(logits, y)\n",
    "            acc_sum += a; prec_sum += p; rec_sum += r; f1_sum += f1\n",
    "    val_loss /= max(1, len(val_loader))\n",
    "    acc = acc_sum / max(1, len(val_loader))\n",
    "    prec = prec_sum / max(1, len(val_loader))\n",
    "    rec = rec_sum / max(1, len(val_loader))\n",
    "    f1 = f1_sum / max(1, len(val_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch} | train_loss={train_loss:.4f} \"\n",
    "          f\"val_loss={val_loss:.4f} acc={acc:.3f} prec={prec:.3f} rec={rec:.3f} f1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pixels: 2076608 positives: 8787 negatives: 2067821 pos_ratio: 0.004231419699818165\n"
     ]
    }
   ],
   "source": [
    "# Check fire prevalence in the mini dataset\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(\"data/minimal_cnn_samples.npz\")\n",
    "y = data[\"y\"]  # [N,H,W], already binarized\n",
    "\n",
    "pos = (y == 1).sum()\n",
    "neg = (y == 0).sum()\n",
    "print(\"Total pixels:\", y.size, \"positives:\", pos, \"negatives:\", neg, \"pos_ratio:\", pos / y.size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
