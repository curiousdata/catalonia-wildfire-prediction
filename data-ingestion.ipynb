{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File to complete data ingestion with PyTorch's `dataset` and `DataLoader` directly from NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(\"data/IberFire.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for spatio-temporal data stored in a NetCDF file.\n",
    "\n",
    "    Each sample:\n",
    "        - inputs: sequence of past time steps (T, C, H, W)\n",
    "        - target: target variable at the *next* time step (H, W)\n",
    "\n",
    "    Spatially, the domain is split into non-overlapping patches of size (patch_size x patch_size).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Path,\n",
    "        sequence_length: int = 30,\n",
    "        stride: int = 1,\n",
    "        patch_size: int = 64,\n",
    "        target_variable: str = \"is_near_fire\",\n",
    "        predictor_variables: Optional[List[str]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_path : Path\n",
    "            Path to the NetCDF file.\n",
    "        sequence_length : int\n",
    "            Number of past time steps T in each input sequence.\n",
    "        stride : int\n",
    "            Step in time between the starts of consecutive sequences.\n",
    "        patch_size : int\n",
    "            Height/width of each square patch (in grid cells).\n",
    "        target_variable : str\n",
    "            Name of the variable to predict at the next time step.\n",
    "        predictor_variables : list of str, optional\n",
    "            Variables to use as input channels.\n",
    "            If None, uses all data variables except the target.\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.patch_size = patch_size\n",
    "        self.target_variable = target_variable\n",
    "\n",
    "        # IMPORTANT: open without chunks → simpler, each __getitem__ does a small read\n",
    "        # You can add engine=\"h5netcdf\" if needed\n",
    "        self.ds = xr.open_dataset(self.data_path, decode_times=True)\n",
    "\n",
    "        # Basic dimension info\n",
    "        self.time_len = int(self.ds.dims[\"time\"])\n",
    "        self.y_len = int(self.ds.dims[\"y\"])\n",
    "        self.x_len = int(self.ds.dims[\"x\"])\n",
    "\n",
    "        # Choose predictor variables\n",
    "        if predictor_variables is None:\n",
    "            self.predictor_variables = [\n",
    "                v for v in self.ds.data_vars if v != self.target_variable\n",
    "            ]\n",
    "        else:\n",
    "            self.predictor_variables = predictor_variables\n",
    "\n",
    "        # Precompute how many patches fit in y/x\n",
    "        self.n_patches_y = self.y_len // self.patch_size  # floor\n",
    "        self.n_patches_x = self.x_len // self.patch_size  # floor\n",
    "\n",
    "        # How many valid time windows?\n",
    "        # We use times [t0, ..., t0+T-1] as inputs, and time t_target = t0+T as target.\n",
    "        # Need t_target < time_len  →  t0 <= time_len - T - 1\n",
    "        max_t0 = self.time_len - self.sequence_length - 1\n",
    "        if max_t0 < 0:\n",
    "            raise ValueError(\"sequence_length is too long for the available time dimension.\")\n",
    "\n",
    "        self.n_time_windows = (max_t0 // self.stride) + 1\n",
    "\n",
    "        # Total samples = time_windows * patches_y * patches_x\n",
    "        self._len = self.n_time_windows * self.n_patches_y * self.n_patches_x\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "\n",
    "    def _index_to_coords(self, idx: int) -> Tuple[int, int, int, slice, slice]:\n",
    "        \"\"\"\n",
    "        Map a flat dataset index to:\n",
    "          - t0, t1: start/end of input sequence in time\n",
    "          - t_target: time index for target (next step after sequence)\n",
    "          - y_slice, x_slice: patch slice in space\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= self._len:\n",
    "            raise IndexError(idx)\n",
    "\n",
    "        patches_per_time = self.n_patches_y * self.n_patches_x\n",
    "\n",
    "        # which time window?\n",
    "        time_idx = idx // patches_per_time\n",
    "        patch_idx = idx % patches_per_time\n",
    "\n",
    "        # time indices\n",
    "        t0 = time_idx * self.stride\n",
    "        t1 = t0 + self.sequence_length\n",
    "        t_target = t1  # next time step after the sequence\n",
    "\n",
    "        # which patch in y/x?\n",
    "        py = patch_idx // self.n_patches_x\n",
    "        px = patch_idx % self.n_patches_x\n",
    "\n",
    "        y0 = py * self.patch_size\n",
    "        y1 = y0 + self.patch_size\n",
    "        x0 = px * self.patch_size\n",
    "        x1 = x0 + self.patch_size\n",
    "\n",
    "        y_slice = slice(y0, y1)\n",
    "        x_slice = slice(x0, x1)\n",
    "\n",
    "        return t0, t1, t_target, y_slice, x_slice\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        t0, t1, t_target, y_slice, x_slice = self._index_to_coords(idx)\n",
    "\n",
    "        # ---- Inputs: (T, C, H, W) ----\n",
    "        # Take selected predictor variables, slice in time and space\n",
    "        # ds[predictor_variables] is a Dataset; to_array() gives DataArray with\n",
    "        # dims: (variable, time, y, x)\n",
    "        window_ds = self.ds[self.predictor_variables].isel(\n",
    "            time=slice(t0, t1),\n",
    "            y=y_slice,\n",
    "            x=x_slice,\n",
    "        )\n",
    "        window_da = window_ds.to_array(\"channel\")  # dims: (channel, time, y, x)\n",
    "        window_da = window_da.transpose(\"time\", \"channel\", \"y\", \"x\")\n",
    "        x_np = window_da.values.astype(\"float32\")  # (T, C, H, W)\n",
    "\n",
    "        # ---- Target: (H, W) ----\n",
    "        target_da = self.ds[self.target_variable].isel(\n",
    "            time=t_target,\n",
    "            y=y_slice,\n",
    "            x=x_slice,\n",
    "        )\n",
    "        y_np = target_da.values.astype(\"float32\")  # (H, W)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        x = torch.from_numpy(x_np)  # (T, C, H, W)\n",
    "        y = torch.from_numpy(y_np)  # (H, W)\n",
    "\n",
    "        sample = {\n",
    "            \"inputs\": x,\n",
    "            \"target\": y,\n",
    "            \"t0\": t0,\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_3744/137488536.py:49: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  self.time_len = int(self.ds.dims[\"time\"])\n",
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_3744/137488536.py:50: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  self.y_len = int(self.ds.dims[\"y\"])\n",
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_3744/137488536.py:51: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  self.x_len = int(self.ds.dims[\"x\"])\n",
      "/Users/vladimir/catalonia-wildfire-prediction/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'temp_2m'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~/catalonia-wildfire-prediction/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, names)\u001b[39m\n\u001b[32m   1116\u001b[39m                 variables[name] = self._variables[name]\n\u001b[32m   1117\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m KeyError:\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m                 ref_name, var_name, var = _get_virtual_variable(\n\u001b[32m   1119\u001b[39m                     self._variables, name, self.sizes\n",
      "\u001b[31mKeyError\u001b[39m: 'temp_2m'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      3\u001b[39m dataset = SpatioTemporalDataset(\n\u001b[32m      4\u001b[39m     data_path=DATASET_PATH,\n\u001b[32m      5\u001b[39m     sequence_length=\u001b[32m16\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     predictor_variables=[\u001b[33m\"\u001b[39m\u001b[33mtemp_2m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwind_speed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhumidity\u001b[39m\u001b[33m\"\u001b[39m],  \u001b[38;5;66;03m# example\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m loader = DataLoader(\n\u001b[32m     13\u001b[39m     dataset,\n\u001b[32m     14\u001b[39m     batch_size=\u001b[32m4\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m].shape)   \u001b[38;5;66;03m# (B, T, C, H, W)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m].shape)   \u001b[38;5;66;03m# (B, H, W)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mSpatioTemporalDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    116\u001b[39m t0, t1, t_target, y_slice, x_slice = \u001b[38;5;28mself\u001b[39m._index_to_coords(idx)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# ---- Inputs: (T, C, H, W) ----\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Take selected predictor variables, slice in time and space\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# ds[predictor_variables] is a Dataset; to_array() gives DataArray with\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# dims: (variable, time, y, x)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m window_ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor_variables\u001b[49m\u001b[43m]\u001b[49m.isel(\n\u001b[32m    123\u001b[39m     time=\u001b[38;5;28mslice\u001b[39m(t0, t1),\n\u001b[32m    124\u001b[39m     y=y_slice,\n\u001b[32m    125\u001b[39m     x=x_slice,\n\u001b[32m    126\u001b[39m )\n\u001b[32m    127\u001b[39m window_da = window_ds.to_array(\u001b[33m\"\u001b[39m\u001b[33mchannel\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# dims: (channel, time, y, x)\u001b[39;00m\n\u001b[32m    128\u001b[39m window_da = window_da.transpose(\u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mchannel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/dataset.py:1277\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1274\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m utils.iterable_of_hashable(key):\n\u001b[32m-> \u001b[39m\u001b[32m1277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_copy_listed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported key-type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/dataset.py:1118\u001b[39m, in \u001b[36mDataset._copy_listed\u001b[39m\u001b[34m(self, names)\u001b[39m\n\u001b[32m   1116\u001b[39m     variables[name] = \u001b[38;5;28mself\u001b[39m._variables[name]\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     ref_name, var_name, var = \u001b[43m_get_virtual_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msizes\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1121\u001b[39m     variables[var_name] = var\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ref_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coord_names \u001b[38;5;129;01mor\u001b[39;00m ref_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dims:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/catalonia-wildfire-prediction/catalonia-wildfire-prediction/.venv/lib/python3.13/site-packages/xarray/core/dataset_utils.py:79\u001b[39m, in \u001b[36m_get_virtual_variable\u001b[39m\u001b[34m(variables, key, dim_sizes)\u001b[39m\n\u001b[32m     77\u001b[39m split_key = key.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(split_key) != \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m     81\u001b[39m ref_name, var_name = split_key\n\u001b[32m     82\u001b[39m ref_var = variables[ref_name]\n",
      "\u001b[31mKeyError\u001b[39m: 'temp_2m'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = SpatioTemporalDataset(\n",
    "    data_path=DATASET_PATH,\n",
    "    sequence_length=16,\n",
    "    stride=1,\n",
    "    patch_size=64,\n",
    "    target_variable=\"is_near_fire\",\n",
    "    predictor_variables=[\"temp_2m\", \"wind_speed\", \"humidity\"],  # example\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,    # keep temporal order\n",
    "    num_workers=0,    # start with 0; you can tune later\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(batch[\"inputs\"].shape)   # (B, T, C, H, W)\n",
    "print(batch[\"target\"].shape)   # (B, H, W)\n",
    "print(batch[\"t0\"])             # starting time indices for each sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
